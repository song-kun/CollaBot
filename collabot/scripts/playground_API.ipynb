{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#success rate exp\n",
    "#generate trajectory without using gazebo\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from util.pc_data_utils import CameraInfo\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import os\n",
    "import copy\n",
    "_PROXY_VARS = [\n",
    "    \"http_proxy\", \"https_proxy\", \"HTTP_PROXY\", \"HTTPS_PROXY\",\n",
    "    \"ALL_PROXY\", \"all_proxy\", \"no_proxy\", \"NO_PROXY\"\n",
    "]\n",
    "\n",
    "for v in _PROXY_VARS:\n",
    "    os.environ.pop(v, None)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pc data\n",
    "\n",
    "down_sample_size = 0.05\n",
    "from util.pc_data_utils import vis_points,merge_clouds,create_point_cloud_from_depth_image,down_sample,mask_to_pc,to_o3d_pcd\n",
    "\n",
    "from util.pc_data_utils import remove_ground\n",
    "\n",
    "from util.pc_seg import objects_seg\n",
    "import torch\n",
    "\n",
    "vis_flag = False\n",
    "#notebook\n",
    "current_path = os.getcwd()\n",
    "intri_path = os.path.join(current_path,'../param/intrinsics.npy')\n",
    "intrinsic = np.load(intri_path)\n",
    "print(intrinsic)\n",
    "factor_depth =  1000\n",
    "camera = CameraInfo(1280.0, 720.0, intrinsic[0][0], intrinsic[1][1], intrinsic[0][2], intrinsic[1][2], factor_depth)\n",
    "\n",
    "#load the img\n",
    "color_img_list = []\n",
    "depth_img_list = []\n",
    "K_list = []\n",
    "camera_pose_list = []\n",
    "\n",
    "img_num = 5\n",
    "data_dir = os.path.join(current_path,'../example/table_chair/')\n",
    "for i in range(1,img_num+1):\n",
    "    color = np.array(Image.open(os.path.join(data_dir, f'robot1_rgb{i}.png'))) \n",
    "    depth = np.array(Image.open(os.path.join(data_dir, f'robot1_depth{i}.png')))\n",
    "    K_inv = np.load(os.path.join(data_dir, f'robot1_K_inv{i}.npy'))\n",
    "    color_img_list.append(color)\n",
    "    depth_img_list.append(depth)\n",
    "    K_list.append(np.linalg.inv(K_inv))\n",
    "    camera_pose_list.append(K_inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmentation for the pc\n",
    "obj_name_list = ['chair']\n",
    "obj_img_dict = {}\n",
    "obj_img_dict[obj_name_list[0]] = [0]\n",
    "\n",
    "now_seg = objects_seg(color_img_list,depth_img_list,camera,obj_name_list)\n",
    "\n",
    "#get the mask of the object\n",
    "now_seg.obj_img_dict = obj_img_dict\n",
    "\n",
    "#get the scene point clouds\n",
    "scene_pc = []\n",
    "scene_color = []\n",
    "for i in range(0,img_num):\n",
    "    now_pc = create_point_cloud_from_depth_image(depth_img_list[i], camera,organized=False)\n",
    "    scene_color.append(color_img_list[i].reshape(-1,3))\n",
    "    scene_pc.append(now_pc)\n",
    "#merge pc\n",
    "scene_pc = merge_clouds(scene_pc, K_list)\n",
    "scene_color = np.concatenate(scene_color, axis=0)\n",
    "\n",
    "\n",
    "#deal with the obj pointclouds\n",
    "obj_pc_dict = {}\n",
    "pc_dict = {}\n",
    "\n",
    "for target_obj in obj_name_list:\n",
    "    clouds, masks = now_seg.get_obj_cloud(target_obj)\n",
    "\n",
    "    #merge the clouds\n",
    "    new_clouds = []\n",
    "    obj_color = np.array([]).reshape(-1,3)\n",
    "    for index, (cloud, mask) in enumerate(zip(clouds, masks)):\n",
    "        img_index = now_seg.obj_img_dict[target_obj][index]\n",
    "        pc,color = mask_to_pc(mask, cloud, color_img_list[img_index])\n",
    "        new_clouds.append(pc)\n",
    "        \n",
    "\n",
    "        obj_color = np.concatenate((obj_color, color), axis=0)\n",
    "        #save pc\n",
    "        np.save(os.path.join(data_dir, f'{target_obj}_pc_{img_index}.npy'), pc)\n",
    "        np.save(os.path.join(data_dir, f'{target_obj}_color_{img_index}.npy'), color)\n",
    "    \n",
    "    pc_dict[target_obj] = []\n",
    "\n",
    "    obj_K_list = [K_list[i] for i in now_seg.obj_img_dict[target_obj]]\n",
    "\n",
    "    #create_world_pc\n",
    "    for now_cloud, now_K in zip(new_clouds, obj_K_list):\n",
    "        k_inv = np.linalg.inv(now_K)\n",
    "        now_cloud =k_inv[0:3,0:3] @ now_cloud.T \n",
    "        now_cloud = now_cloud.T + k_inv[0:3,3]\n",
    "        pc_dict[target_obj].append(now_cloud)\n",
    "\n",
    "    obj_pc = merge_clouds(new_clouds, obj_K_list)\n",
    "    #remove ground\n",
    "    scene_pc, scene_color, obj_pc, obj_color = remove_ground(scene_pc, scene_color, obj_pc, obj_color)\n",
    "    #down sample\n",
    "    obj_pc, obj_color = down_sample(obj_pc, obj_color, down_sample_size)\n",
    "    \n",
    "    #obj_pcd\n",
    "    obj_pcd = to_o3d_pcd(obj_pc, obj_color)\n",
    "    obj_pc_dict[target_obj] = obj_pcd\n",
    "    \n",
    "#down sample\n",
    "scene_pc, scene_color = down_sample(scene_pc, scene_color, down_sample_size)\n",
    "scene_pcd = to_o3d_pcd(scene_pc, scene_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.pc_data_utils import show_pc\n",
    "o3d.visualization.draw_geometries([obj_pc_dict[\"chair\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#down sample point\n",
    "from util.pc_data_utils import remove_obj_from_scene,write_pc\n",
    "\n",
    "for now_obj in obj_name_list:\n",
    "    obj_pcd = obj_pc_dict[now_obj]\n",
    "    scene_pcd = remove_obj_from_scene(scene_pcd, obj_pcd)\n",
    "\n",
    "\n",
    "#now you get two pointclouds, scene_pc and obj_pc, scene_color and obj_color\n",
    "#save the pointclouds\n",
    "\n",
    "save_path = os.path.join(data_dir, 'scene.ply')\n",
    "print(save_path)\n",
    "o3d.io.write_point_cloud(save_path, scene_pcd)\n",
    "\n",
    "for now_obj in obj_name_list:\n",
    "    save_path = os.path.join(data_dir, f'{now_obj}.ply')\n",
    "    o3d.io.write_point_cloud(save_path, obj_pc_dict[now_obj])\n",
    "del now_seg\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide robot number\n",
    "from util.prompt_interface import prompt_interface\n",
    "import json\n",
    "\n",
    "# obj_list = [\"chair\"]\n",
    "now_obj_name = \"chair\" #object need to manipulate\n",
    "max_robot_num = 2\n",
    "\n",
    "now_instruction = \"Move the chair near the table\"\n",
    "api_type = \"qwen\"\n",
    "#qwen-vl-plus,qwen-vl-max-2025-04-08\n",
    "vlm_model = \"qwen-vl-max-2025-04-08\" #we provide an example using QWEN to achieve this task, because you can try QWEN for free\n",
    "api_file_path = os.path.join(current_path, f'../prompts/api_{api_type}.json')\n",
    "with open(api_file_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "api_key = config[\"api_key\"]\n",
    "api_base = config[\"api_base\"]\n",
    "\n",
    "prompt_inter = prompt_interface(api_key,api_base)\n",
    "\n",
    "prompt_path = os.path.join(current_path, '../prompts/choose_robot_num_prompts.txt')\n",
    "with open(prompt_path, \"r\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "applied_robot_num = prompt_inter.choose_robot_num(color_img_list[obj_img_dict[now_obj_name][0]], prompt, now_instruction, max_robot_num=max_robot_num, model=vlm_model)\n",
    "\n",
    "robot_list = [f\"robot{i+1}\" for i in range(applied_robot_num)]\n",
    "\n",
    "if applied_robot_num != 2:\n",
    "    print(\"The output of VLM is not 2, it will lead to some error in the following process.\")\n",
    "    print(\"Your can directly change applied_robot_num to 2, and generate robot_list again.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If failed, uncomment and run this cell\n",
    "applied_robot_num = 2\n",
    "\n",
    "robot_list = [f\"robot{i+1}\" for i in range(applied_robot_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.draw_axis import draw_axis_on_img\n",
    "\n",
    "img_with_axis = draw_axis_on_img(copy.deepcopy(color_img_list[obj_img_dict[now_obj_name][0]]), K_list[obj_img_dict[now_obj_name][0]], intrinsic)\n",
    "\n",
    "prompt_path = os.path.join(current_path, '../prompts/generate_constraint_prompts.txt')\n",
    "with open(prompt_path, \"r\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "response, obtained_constraint = prompt_inter.obtain_constraint(img_with_axis, prompt, now_instruction, max_robot_num=max_robot_num, model=vlm_model)\n",
    "\n",
    "if \"keep_z_axis\" in obtained_constraint:\n",
    "    keep_z_axis_flag = True\n",
    "else:\n",
    "    keep_z_axis_flag = False\n",
    "print(keep_z_axis_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keep_z_axis_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.funcs import process_pcd\n",
    "import numpy as np\n",
    "\n",
    "#motion planning\n",
    "\n",
    "obj_center = np.mean(obj_pc, axis=0)\n",
    "\n",
    "\n",
    "obj_pose = np.eye(4)\n",
    "scene,obj_list,T_world_2_obj_list = process_pcd(scene_pcd,[obj_pc_dict[now_obj_name]],[obj_pose])\n",
    "\n",
    "now_obj_pcd = obj_list[0]\n",
    "T_world_2_obj = T_world_2_obj_list[0]\n",
    "\n",
    "now_obj_points = np.array(now_obj_pcd.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motion_planning.motion_planning import motion_planner_agv,xyzrpy_to_T\n",
    "from grasping.create_object_grasp_pose import large_obj_grasping\n",
    "\n",
    "\n",
    "\n",
    "#SETP 1: generate a feasible pose for the grasp pose\n",
    "print(\"grasp pose generation\")\n",
    "\n",
    "this_obj_img_list = [color_img_list[index] for index in obj_img_dict[now_obj_name]]\n",
    "this_obj_pc_list = pc_dict[now_obj_name]\n",
    "this_obj_camera_pose_list = [camera_pose_list[index] for index in obj_img_dict[now_obj_name]]\n",
    "gripper_model_path = os.path.join(current_path, 'grasping')\n",
    "\n",
    "large_obj_GP_generator = large_obj_grasping(this_obj_img_list,this_obj_pc_list,this_obj_camera_pose_list,\n",
    "                                            n_robot=applied_robot_num,VLM_model=vlm_model,gripper_CAD_path=gripper_model_path)\n",
    "\n",
    "\n",
    "img_index = 0 #max value len(this_obj_pc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.funcs import generate_ground_mesh\n",
    "ground_z = np.min(scene_pc[:,2])\n",
    "collision_points_obj = large_obj_GP_generator.merged_pc\n",
    "ground_mesh = generate_ground_mesh(collision_points_obj, ground_z)\n",
    "\n",
    "collision_points = np.vstack((collision_points_obj, ground_mesh))\n",
    "print(collision_points_obj.shape,ground_mesh.shape,collision_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_poses,feasible_flag = large_obj_GP_generator.genera_feasible_grasp_poses(img_index,collision_points=collision_points,vis_grasp_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"choose grasp poses\")\n",
    "feasible_grasp_poses = [grasp_poses[i] for i in range(len(grasp_poses)) if feasible_flag[i]]\n",
    "choose_index_list = large_obj_GP_generator.choose_grasp_pose(img_index,feasible_grasp_poses,now_instruction) #using VLM to choose\n",
    "print(choose_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_grasp_poses = [feasible_grasp_poses[i] for i in choose_index_list]\n",
    "grasp_pose = large_obj_GP_generator.collaborative_ik_solver(choose_grasp_poses,collision_points=collision_points,max_potential_q=40)\n",
    "\n",
    "grasp_pose = np.array(grasp_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: motion planning from robot current pose to grasp pose\n",
    "agv_pose_list = np.array([[ 1.00407003, -2.00377211,  0.00944374,  0,  0 ,0,   0,   0, 0 ], [ 0.00714296 , 1.00136292 ,-0.0084044 ,  0,  0, 0, 0 ,0, 0]])\n",
    "planner = motion_planner_agv(agv_pose_list)\n",
    "planner.assign_scene_obj(scene,now_obj_pcd)\n",
    "#update the agv pose\n",
    "for i in range(len(agv_pose_list)):\n",
    "    planner.agv_list[i].move_agv(agv_pose_list[i])\n",
    "# planner.dis_tolerance = 0.01\n",
    "success, robot_pose1 = planner.motion_planning_RRT(grasp_pose,T_world_2_obj,total_points_num=500) #motion planning will influence the robot_pose\n",
    "\n",
    "test_T = xyzrpy_to_T([T_world_2_obj[0,3],T_world_2_obj[1,3],T_world_2_obj[2,3],0,0,0])\n",
    "planner.add_grasp(test_T) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  success:\n",
    "    print(f\"success in grasping process with {len(robot_pose1)} steps\")\n",
    "    obj_pose_list = [T_world_2_obj for i in range(len(robot_pose1))]\n",
    "    planner.animation(robot_pose1,dt=0.005,scene_pcd=scene,object_list=[now_obj_pcd],obj_pose_list=obj_pose_list, stop_time=1)\n",
    "else:\n",
    "    print(\"failed in grasping process\")\n",
    "    planner.animation(robot_pose1,scene_pcd=scene,object_list=[now_obj_pcd], stop_time=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3: planning for the object\n",
    "\n",
    "start = np.array([T_world_2_obj[0,3],T_world_2_obj[1,3],T_world_2_obj[2,3],0,0,0]) #x,y,z,r,p,y\n",
    "goal = np.array([T_world_2_obj[0,3]+1,T_world_2_obj[1,3]-3,T_world_2_obj[2,3],0,0,-1.57]) #move to origin\n",
    "\n",
    "if keep_z_axis_flag:\n",
    "    simplifed_start = np.array([start[0], start[1], start[2], start[5]]) #keep z axis\n",
    "    simplifed_goal = np.array([goal[0], goal[1], goal[2], goal[5]])\n",
    "    success, path = planner.obj_planning_keepz(simplifed_start,simplifed_goal,n_point=500) #we only consider the situation of keep z axis, which is most common in the real world\n",
    "    new_path = []\n",
    "    for now_point in path:\n",
    "        new_point = np.array([now_point[0], now_point[1], now_point[2], start[3], start[4], now_point[3]])\n",
    "        new_path.append(new_point)\n",
    "    path = new_path\n",
    "else:\n",
    "    success, path = planner.obj_planning(start,goal,n_point=500)\n",
    "# path = [goal]\n",
    "obj_path_T = [xyzrpy_to_T(p,xyz_first=True) for p in path] #in world frame\n",
    "if success:\n",
    "    print(f\"find object path with {len(path)} points\")\n",
    "\n",
    "#STEP 4: motion planning for each robot\n",
    "#set the robots' pose to grasp pose\n",
    "for i in range(len(robot_list)):\n",
    "    now_grasp_pose = grasp_pose[i]\n",
    "    planner.agv_list[i].FK(now_grasp_pose)\n",
    "success, robot_pose2 = planner.motion_planning(obj_path_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  success:\n",
    "    print(\"success in planning for each robot\")\n",
    "    planner.animation(robot_pose2,dt = 0.005,scene_pcd=scene,object_list=[now_obj_pcd],obj_pose_list=obj_path_T,stop_time=2)\n",
    "else:\n",
    "    print(\"------------failed in planning for each robot------------\")\n",
    "    planner.animation(robot_pose2,scene_pcd=scene,object_list=[now_obj_pcd],obj_pose_list=obj_path_T,stop_time=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
